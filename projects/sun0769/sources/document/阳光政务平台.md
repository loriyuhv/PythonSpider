# <center>阳光政务平台</center>

## 1. 准备

- **在pycharm的terminal中执行创建scrapy文件**

  ```python
  scrapy startproject sun0769
  cd sun0769
  scrapy genspider sun wz.sun0769.com
  ```

  ![01](D:\Study\Python\PythonSpider\projects\sun0769\sources\document\source\01.png)

  **文件**

  ![02](D:\Study\Python\PythonSpider\projects\sun0769\sources\document\source\02.png)

- **配置```settings.py```**

  ```python
  # 遵循robots协议 True：开启，False：关闭
  ROBOTSTXT_OBEY = False
  
  # 日志信息屏蔽
  LOG_LEVEL = "WARNING"
  
  # 添加请求头
  import random
  user_agent = [
      "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1",
      "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6",
      "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6",
      "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1",
      "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5",
      "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
      "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
      "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
      "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
      "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
      "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
      "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
      "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3",
      "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"
  ]
  DEFAULT_REQUEST_HEADERS = {
      "User-Agent": random.choice(user_agent)
  }
  
  # 开启管道模式
  ITEM_PIPELINES = {
     'sun0769.pipelines.Sun0769Pipeline': 300,
  }
  ```


## 2. coding

- **构造请求并且解析网页```sun.py```**

  ```python
  import scrapy
  from scrapy import Request
  from urllib.parse import urlencode
  
  
  class SunSpider(scrapy.Spider):
      name = 'sun'    # 爬虫名称
      MAX_PAGE = 10   # 最大爬取页数
  
      def start_requests(self):
          # 基础url地址
          base_url = "https://wz.sun0769.com/political/index/politicsNewest?"
  
          # 通过循环，构造10页的请求request
          for page in range(1, self.MAX_PAGE+1):
              data = {
                  "id": "1",
                  "page": page
              }
              params = urlencode(data)
              url = base_url + params     # 完整的url由 基础的url + 参数拼接而成
              # print(url)    # 查看构造的url
              yield Request(url, self.parse)
  
      def parse(self, response, **kwargs):
          # 解析页面
          banks = response.xpath('//div[@class="width-12"]/ul/li[@class="clear"]')
          item = {}
          for bank in banks:
              # 获取编号
              item["number"] = bank.xpath('./span[@class="state1"]/text()').extract_first().strip()
              # 获取状态
              item["status"] = bank.xpath('./span[@class="state2"]/text()').extract_first().strip()
              # 获取问政标题
              item["title"] = bank.xpath('./span[@class="state3"]/a/text()').extract_first().strip()
              # 获取响应时间
              item["resp_time"] = bank.xpath('./span[@class="state4"]/text()').extract_first().strip()
              # 获取问政时间
              item["req_time"] = bank.xpath('./span[@class="state5 "]/text()').extract_first()
              # 获取详细信息链接
              item["link"] = "https://wz.sun0769.com" + bank.xpath('./span[@class="state3"]/a/@href').extract_first().strip()
              # 传入管道，item类型是dict
              yield item
  ```

- **存储```pipelines.py```**

  ```python
  from itemadapter import ItemAdapter
  import json
  import csv
  import pymysql
  import os
  
  
  class Sun0769Pipeline:
      
      def __init__(self):
          # 获取当前目录路径
          # self.current_path = os.getcwd()
          # 获取上一级目录路径
          self.up_path = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))
  
      def process_item(self, item, spider):
          if spider.name == "sun":
              # 写入CSV，文件名：data
              # self.write_csv(item, name="data")
              # 写入JSON，文件名：data
              # self.write_json(item, name="data")
              # 写入mysql
              self.write_mysql(item)
          return item
  
      def write_mysql(self, item):
          # 链接数据库，创建对象db
          db = pymysql.connect(
              host="localhost",
              user="root",
              password="12345",
              database="test03",
              port=3306
          )
          # 创建游标
          cursor = db.cursor()
          
          # 创建数据库data
          sql = "CREATE TABLE IF NOT EXISTS data (" \
                "`编号` VARCHAR(255), `状态` VARCHAR(255)," \
                "`问政问题` VARCHAR(255), `响应时间` VARCHAR(255)," \
                "`问政时间` VARCHAR(255), `链接` VARCHAR(255)" \
                ")"
          cursor.execute(sql)
          
          # 插入数据
          sql01 = "INSERT INTO data(`编号`, `状态`, `问政问题`, `响应时间`, `问政时间`, `链接`) VALUES(%s, %s, %s, %s, %s, %s)"
          cursor.execute(sql01, (item["number"], item["status"],
                                 item["title"], item["resp_time"], item["req_time"], item["link"]))
          # 提交命令
          db.commit()
          
          # 关闭数据库
          db.close()
  
      def write_json(self, item, name):
          path = self.up_path + "/sources/data/%s.json" % name
          with open(path, mode='at', encoding='utf-8') as f:
              f.write(json.dumps(item, ensure_ascii=False) + ",\n")
  
      def write_csv(self, item, name):
          file_full_path = self.up_path + "/sources/data/%s.csv" % name
          if os.path.exists(file_full_path):
              with open(file_full_path, mode="at", encoding='gbk') as f:
                  writer = csv.writer(f)
                  writer.writerow([item["number"], item["status"], item["title"],
                                   item["resp_time"], item["req_time"], item["link"]])
          else:
              with open(file_full_path, mode='wt', encoding='gbk') as f:
                  writer = csv.writer(f)
                  writer.writerow(["编号", "状态", "问政问题", "响应时间", "问政时间", "链接"])
              with open(file_full_path, mode="at", encoding='gbk') as f:
                  writer = csv.writer(f)
                  writer.writerow([item["number"], item["status"], item["title"],
                                   item["resp_time"], item["req_time"], item["link"]])
  
  ```

